#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from rclpy.executors import MultiThreadedExecutor
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped, Point32
from geometry_msgs.msg import PolygonStamped
from cv_bridge import CvBridge
import cv2
import numpy as np
import time
from typing import List, Dict, Optional, Tuple
from collections import deque
import copy

# å¯¼å…¥RDK YOLOæ¨¡å‹å’ŒBOTSORTè·Ÿè¸ªå™¨
from .YOLOv8_Hand_Detect import YOLOv8_Detect
from .BOTSort_rdk import BOTSORT, OSNetReID


class TrackedTarget:
    """è·Ÿè¸ªç›®æ ‡ä¿¡æ¯ç±» - ä½¿ç”¨__slots__ä¼˜åŒ–å†…å­˜"""
    __slots__ = ('track_id', 'bbox', 'feature', 'height_pixels', 'first_seen_time', 
                 'last_seen_time', 'last_update_time', 'lost_frames', 'is_recovered', 
                 'is_switched', 'original_track_id', 'recovery_time', 'update_paused')
    
    def __init__(self, track_id: int, bbox: List[float], feature: np.ndarray, 
                 height_pixels: float, timestamp: float):
        self.track_id = track_id
        self.bbox = bbox
        self.feature = feature
        self.height_pixels = height_pixels
        self.first_seen_time = timestamp
        self.last_seen_time = timestamp
        self.last_update_time = timestamp
        self.lost_frames = 0
        self.is_recovered = False
        self.is_switched = False
        self.original_track_id = track_id
        self.recovery_time = None
        self.update_paused = False
    
    def update(self, bbox: List[float], feature: np.ndarray, height_pixels: float, timestamp: float):
        """æ›´æ–°ç›®æ ‡ä¿¡æ¯"""
        self.bbox = bbox
        self.feature = feature
        self.height_pixels = height_pixels
        self.last_seen_time = timestamp
        self.last_update_time = timestamp
        self.lost_frames = 0
        self.is_recovered = False
    
    def mark_lost(self):
        """æ ‡è®°ç›®æ ‡ä¸¢å¤±"""
        self.lost_frames += 1
        self.update_paused = True
        self.recovery_time = None
    
    def mark_recovered(self, timestamp: float):
        """æ ‡è®°ç›®æ ‡æ‰¾å› - å–æ¶ˆå†·å´æœŸï¼Œç«‹å³æ¢å¤æ­£å¸¸æ›´æ–°"""
        self.is_recovered = True
        self.lost_frames = 0
        self.recovery_time = None  # ä¸å†è®¾ç½®æ¢å¤æ—¶é—´
        self.update_paused = False  # ç«‹å³ç»“æŸæš‚åœçŠ¶æ€
    
    def switch_to_new_id(self, new_track_id: int):
        """åˆ‡æ¢åˆ°æ–°çš„è·Ÿè¸ªID"""
        self.is_switched = True
        self.track_id = new_track_id

class Yolov8HandTrackNode(Node):
    def __init__(self):
        super().__init__('yolov8_hand_track_node')

        # å£°æ˜å‚æ•°
        self._declare_parameters()
        
        # è·å–å‚æ•°
        self._get_parameters()
        
        self.min_process_interval = 1.0 / self.max_processing_fps
        self.last_process_time = time.time()

        # åˆå§‹åŒ–æ¨¡å‹å’Œç»„ä»¶
        self._initialize_components()
        
        # åˆå§‹åŒ–å˜é‡
        self._initialize_variables()
        
        self.get_logger().info("YOLOv8 Hand Track Node initialized with ReID recovery (Optimized)")
        self.print_parameters()

    def _declare_parameters(self):
        """å£°æ˜æ‰€æœ‰å‚æ•°"""
        self.declare_parameter('model_path', '')
        self.declare_parameter('conf_threshold', 0.3)
        self.declare_parameter('max_processing_fps', 15)
        self.declare_parameter('ok_confirm_frames', 3)
        self.declare_parameter('tracking_protection_time', 5.0)
        self.declare_parameter('reid_similarity_threshold', 0.8)
        self.declare_parameter('height_change_threshold', 0.15)
        self.declare_parameter('lost_timeout_threshold', 10.0)
        self.declare_parameter('reid_model_path', 'osnet_64x128_nv12.bin')
        self.declare_parameter('roi_threshold', 0.5)  # é™ä½IoUé˜ˆå€¼
        
    def _get_parameters(self):
        """è·å–å‚æ•°å€¼"""
        self.conf_threshold = self.get_parameter('conf_threshold').value
        self.max_processing_fps = self.get_parameter('max_processing_fps').value
        self.ok_confirm_frames = self.get_parameter('ok_confirm_frames').value
        self.tracking_protection_time = self.get_parameter('tracking_protection_time').value
        self.reid_similarity_threshold = self.get_parameter('reid_similarity_threshold').value
        self.height_change_threshold = self.get_parameter('height_change_threshold').value
        self.lost_timeout_threshold = self.get_parameter('lost_timeout_threshold').value
        self.roi_threshold = self.get_parameter('roi_threshold').value

    def print_parameters(self):
        """æ‰“å°å‚æ•°ä¿¡æ¯"""
        self.get_logger().info("===== å‚æ•°é…ç½®ä¿¡æ¯ =====")
        self.get_logger().info(f"ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold}")
        self.get_logger().info(f"æœ€å¤§å¤„ç†å¸§ç‡: {self.max_processing_fps}FPS")
        self.get_logger().info(f"OKæ‰‹åŠ¿ç¡®è®¤å¸§æ•°: {self.ok_confirm_frames}")
        self.get_logger().info(f"è·Ÿè¸ªä¿æŠ¤æ—¶é—´: {self.tracking_protection_time}s")
        self.get_logger().info(f"ReIDç›¸ä¼¼åº¦é˜ˆå€¼: {self.reid_similarity_threshold}")
        self.get_logger().info(f"é«˜åº¦å˜åŒ–é˜ˆå€¼: {self.height_change_threshold}")
        self.get_logger().info(f"ä¸¢å¤±è¶…æ—¶é˜ˆå€¼: {self.lost_timeout_threshold}s")
        self.get_logger().info(f"ROIé‡å é˜ˆå€¼: {self.roi_threshold}")
        self.get_logger().info("=========================")

    def _initialize_components(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œè·Ÿè¸ªå™¨"""
        model_path = self.get_parameter('model_path').value
        reid_model_path = self.get_parameter('reid_model_path').value
        
        # åŠ è½½YOLOv8 hand detectæ¨¡å‹
        self.model = YOLOv8_Detect(model_path, self.conf_threshold, 0.45, 3, 16)
        
        # åˆå§‹åŒ–ReIDç¼–ç å™¨
        self.reid_encoder = None
        try:
            self.reid_encoder = OSNetReID(reid_model_path)
            self.get_logger().info(f"ReIDæ¨¡å‹åŠ è½½æˆåŠŸ: {reid_model_path}")
        except Exception as e:
            self.get_logger().error(f"ReIDæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–BOTSORTè·Ÿè¸ªå™¨
        tracker_args = {
            'track_high_thresh': 0.25,
            'track_low_thresh': 0.1,
            'new_track_thresh': 0.25,
            'track_buffer': 10,
            'match_thresh': 0.68,
            'fuse_score': False,
            'gmc_method': 'sparseOptFlow',
            'proximity_thresh': 0.5,
            'appearance_thresh': 0.7,
            'with_reid': False,
            'reid_model_path': reid_model_path
        }
        self.tracker = BOTSORT(tracker_args)
        
        # åˆå§‹åŒ–CV bridge
        self.bridge = CvBridge()
        
        # åˆ›å»ºè®¢é˜…å’Œå‘å¸ƒ
        self.image_sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)
        self.detect_pose_pub = self.create_publisher(Image, 'tracks', 10)
        self.keypoint_tracks_pub = self.create_publisher(PolygonStamped, '/keypoint_tracks', 10)

    def _initialize_variables(self):
        """åˆå§‹åŒ–å˜é‡"""
        # è·Ÿè¸ªç›¸å…³å˜é‡
        self.tracked_persons: Dict[int, Dict] = {}

        # æ‰‹åŠ¿æ£€æµ‹å†å²
        self.ok_gesture_history: Dict[int, deque] = {}
        self.stop_gesture_history: Dict[int, deque] = {}
        
        # å½“å‰æ­£åœ¨è·Ÿè¸ªçš„ç›®æ ‡ID
        self.current_tracking_id = None
        
        # è·Ÿè¸ªç›®æ ‡ä¿¡æ¯å­˜å‚¨
        self.tracked_targets: Dict[int, TrackedTarget] = {}
        
        # ç›®æ ‡ä¸¢å¤±æ—¶é—´è®°å½•
        self.target_lost_time: Optional[float] = None
        
        # ç±»åˆ«åç§°æ˜ å°„
        self.class_names = {0: "person", 1: "ok", 2: "stop"}

    def calculate_iou(self, box1, box2):
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # è®¡ç®—äº¤é›†åŒºåŸŸ
        inter_x1 = max(x1_1, x1_2)
        inter_y1 = max(y1_1, y1_2)
        inter_x2 = min(x2_1, x2_2)
        inter_y2 = min(y2_1, y2_2)
        
        # è®¡ç®—äº¤é›†é¢ç§¯
        inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
        
        # è®¡ç®—å¹¶é›†é¢ç§¯
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        # è®¡ç®—IoU
        iou = inter_area / union_area if union_area > 0 else 0
        return iou

    def find_person_for_gesture(self, gesture_box, person_boxes):
        """ä¸ºæ‰‹åŠ¿æ‰¾åˆ°å¯¹åº”çš„äººä½“è¾¹ç•Œæ¡† - ä½¿ç”¨ä¸­å¿ƒç‚¹åŒ¹é…"""
        gx1, gy1, gx2, gy2 = gesture_box
        gesture_center_x = (gx1 + gx2) / 2
        gesture_center_y = (gy1 + gy2) / 2
        
        best_person_box = None
        best_person_id = None
        min_distance = float('inf')
        
        for person_id, person_box in person_boxes.items():
            px1, py1, px2, py2 = person_box
            
            # æ£€æŸ¥æ‰‹åŠ¿ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨äººä½“æ¡†å†…
            if (px1 <= gesture_center_x <= px2 and 
                py1 <= gesture_center_y <= py2):
                
                # è®¡ç®—ä¸­å¿ƒç‚¹åˆ°äººä½“æ¡†ä¸­å¿ƒçš„è·ç¦»
                person_center_x = (px1 + px2) / 2
                person_center_y = (py1 + py2) / 2
                distance = ((gesture_center_x - person_center_x) ** 2 + 
                        (gesture_center_y - person_center_y) ** 2) ** 0.5
                
                self.get_logger().info(f"æ‰‹åŠ¿ä¸­å¿ƒåœ¨ID {person_id} æ¡†å†…ï¼Œè·ç¦»: {distance:.1f}")
                
                if distance < min_distance:
                    min_distance = distance
                    best_person_box = person_box
                    best_person_id = person_id
        
        # å¦‚æœæ‰¾åˆ°åŒ…å«æ‰‹åŠ¿ä¸­å¿ƒç‚¹çš„äººä½“æ¡†ï¼Œç›´æ¥è¿”å›
        if best_person_id is not None:
            return best_person_id, best_person_box, 1.0  # é‡å æ¯”ä¾‹ä¸º1.0
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨åŸæ¥çš„IoUæ–¹æ³•ä½œä¸ºå¤‡é€‰
        return self.find_person_for_gesture_fallback(gesture_box, person_boxes)

    def find_person_for_gesture_fallback(self, gesture_box, person_boxes):
        """å¤‡é€‰æ–¹æ³•ï¼šä½¿ç”¨é‡å æ¯”ä¾‹"""
        best_overlap = 0
        best_person_box = None
        best_person_id = None
        
        gx1, gy1, gx2, gy2 = gesture_box
        gesture_area = (gx2 - gx1) * (gy2 - gy1)
        
        for person_id, person_box in person_boxes.items():
            px1, py1, px2, py2 = person_box
            
            # è®¡ç®—äº¤é›†é¢ç§¯
            inter_x1 = max(gx1, px1)
            inter_y1 = max(gy1, py1)
            inter_x2 = min(gx2, px2)
            inter_y2 = min(gy2, py2)
            
            inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
            overlap = inter_area / gesture_area if gesture_area > 0 else 0
            
            if overlap > best_overlap:
                best_overlap = overlap
                best_person_box = person_box
                best_person_id = person_id
        
        return best_person_id, best_person_box, best_overlap

    def extract_feature_from_bbox(self, image: np.ndarray, bbox: List[float]) -> np.ndarray:
        """ä»è¾¹ç•Œæ¡†æå–ç‰¹å¾ - ä¼˜åŒ–å†…å­˜åˆ†é…"""
        x1, y1, x2, y2 = map(int, bbox)
        h, w = image.shape[:2]
        
        # è¾¹ç•Œæ£€æŸ¥
        x1 = max(0, min(x1, w - 1))
        y1 = max(0, min(y1, h - 1))
        x2 = max(0, min(x2, w - 1))
        y2 = max(0, min(y2, h - 1))
        
        if x2 <= x1 or y2 <= y1:
            return np.zeros(512, dtype=np.float32)
        
        crop = image[y1:y2, x1:x2]
        if crop.size == 0:
            return np.zeros(512, dtype=np.float32)
        
        try:
            if self.reid_encoder is not None:
                return self.reid_encoder.extract_feature(crop)
            else:
                return np.zeros(512, dtype=np.float32)
        except Exception as e:
            self.get_logger().warn(f"Feature extraction failed: {e}")
            return np.zeros(512, dtype=np.float32)
        
    def save_tracked_target(self, track_id: int, bbox: List[float], image: np.ndarray, timestamp: float):
        """ä¿å­˜è·Ÿè¸ªç›®æ ‡ä¿¡æ¯ - æ¯å¸§æ›´æ–°æ‰€æœ‰ç‰¹å¾ä¿¡æ¯"""
        if track_id not in self.tracked_targets:
            # æ–°ç›®æ ‡ï¼šæå–ç‰¹å¾
            feature = self.extract_feature_from_bbox(image, bbox)
            height_pixels = bbox[3] - bbox[1]
            self.tracked_targets[track_id] = TrackedTarget(track_id, bbox, feature, height_pixels, timestamp)
            return
        
        target = self.tracked_targets[track_id]
        
        # å¦‚æœæ›´æ–°è¢«æš‚åœï¼ˆä»…ç”¨äºå…¶ä»–æš‚åœæƒ…å†µï¼‰ï¼Œè·³è¿‡ç‰¹å¾æ›´æ–°
        if target.update_paused:
            target.bbox = bbox
            target.height_pixels = bbox[3] - bbox[1]
            target.last_seen_time = timestamp
            target.lost_frames = 0
            return
        
        # æ¯å¸§éƒ½æ›´æ–°ç‰¹å¾ï¼ˆé«˜ç²¾åº¦æ¨¡å¼ï¼‰
        feature = self.extract_feature_from_bbox(image, bbox)
        height_pixels = bbox[3] - bbox[1]
        target.update(bbox, feature, height_pixels, timestamp)

    def try_recover_lost_target(self, current_tracks: List[Dict], image: np.ndarray, timestamp: float) -> Optional[int]:
        """ç«‹å³å°è¯•æ‰¾å›ä¸¢å¤±çš„è·Ÿè¸ªç›®æ ‡ - å¢åŠ é«˜åº¦ç­›é€‰"""
        if self.current_tracking_id is None or self.current_tracking_id not in self.tracked_targets:
            return None
        
        target = self.tracked_targets[self.current_tracking_id]
        
        # ç«‹å³è®°å½•ä¸¢å¤±æ—¶é—´
        if self.target_lost_time is None:
            self.target_lost_time = timestamp
            self.get_logger().info(f"ç›®æ ‡ {self.current_tracking_id} ä¸¢å¤±ï¼Œå¼€å§‹ç«‹å³ReIDåŒ¹é…æ‰¾å›")
        
        # å‡†å¤‡å€™é€‰ç›®æ ‡
        candidate_tracks = []
        for track in current_tracks:
            track_id = track['track_id']
            is_currently_tracked = (
                track_id in self.tracked_persons and 
                self.tracked_persons[track_id]['is_tracking'] and
                track_id != self.current_tracking_id
            )
            
            if not is_currently_tracked:
                candidate_tracks.append(track)
        
        if not candidate_tracks:
            return None
        
        # è·å–ä¸¢å¤±ç›®æ ‡çš„é«˜åº¦ä¿¡æ¯
        target_height_pixels = target.height_pixels
        self.get_logger().info(f"ç›®æ ‡ {self.current_tracking_id} ä¸¢å¤±æ—¶é«˜åº¦: {target_height_pixels:.1f}px")
        
        # ReIDåŒ¹é…
        best_match_id = None
        best_similarity = 0.0
        
        for track in candidate_tracks:
            track_id = track['track_id']
            bbox = track['bbox']
            
            # è®¡ç®—å½“å‰å€™é€‰ç›®æ ‡çš„é«˜åº¦
            x1, y1, x2, y2 = bbox
            candidate_height_pixels = y2 - y1
            
            # é«˜åº¦å˜åŒ–ç­›é€‰
            height_ratio = candidate_height_pixels / target_height_pixels
            height_change = abs(1.0 - height_ratio)
            
            # å¦‚æœé«˜åº¦å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œè·³è¿‡è¯¥å€™é€‰ç›®æ ‡
            if height_change > self.height_change_threshold:
                self.get_logger().warning(f"å€™é€‰ç›®æ ‡ ID:{track_id} é«˜åº¦å˜åŒ– {height_change:.3f} è¶…è¿‡é˜ˆå€¼ {self.height_change_threshold}, è·³è¿‡åŒ¹é…")
                continue
            
            candidate_feature = self.extract_feature_from_bbox(image, bbox)
            
            if candidate_feature is not None and np.any(candidate_feature):
                similarity = np.dot(target.feature, candidate_feature) / (
                    np.linalg.norm(target.feature) * np.linalg.norm(candidate_feature) + 1e-8
                )
                
                # è®°å½•åŒ¹é…ä¿¡æ¯ï¼ˆåŒ…æ‹¬é«˜åº¦ä¿¡æ¯ï¼‰
                self.get_logger().info(f"å€™é€‰ç›®æ ‡ ID:{track_id} ReIDç›¸ä¼¼åº¦: {similarity:.3f}, é«˜åº¦å˜åŒ–: {height_change:.3f}")
                
                if similarity >= self.reid_similarity_threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_match_id = track_id
        
        if best_match_id is not None:
            self.get_logger().info(
                f"ç›®æ ‡ {self.current_tracking_id} ReIDæ‰¾å›æˆåŠŸ! åŒ¹é…ID: {best_match_id}, ç›¸ä¼¼åº¦: {best_similarity:.3f}"
            )
            
            target_bbox = next(t['bbox'] for t in candidate_tracks if t['track_id'] == best_match_id)
            target.mark_recovered(timestamp)
            
            # é‡è¦ä¿®æ”¹ï¼šæ‰¾å›åç«‹å³ç»“æŸæš‚åœçŠ¶æ€ï¼Œæ¢å¤æ­£å¸¸ç‰¹å¾æ›´æ–°
            target.update_paused = False
            target.recovery_time = None
            
            # ç«‹å³ä¿å­˜ç›®æ ‡ä¿¡æ¯ï¼ˆæ¯å¸§æ›´æ–°ç‰¹å¾ï¼‰
            self.save_tracked_target(self.current_tracking_id, target_bbox, image, timestamp)
            
            recovered_id = best_match_id
            
            if best_match_id != self.current_tracking_id:
                if best_match_id in self.tracked_targets:
                    self.tracked_targets[best_match_id].switch_to_new_id(best_match_id)
                    self.tracked_targets[best_match_id].original_track_id = self.current_tracking_id
                    self.tracked_targets[best_match_id].mark_recovered(timestamp)
                    # åŒæ ·ç»“æŸæ–°ç›®æ ‡çš„æš‚åœçŠ¶æ€
                    self.tracked_targets[best_match_id].update_paused = False
                    self.tracked_targets[best_match_id].recovery_time = None
            
            self.target_lost_time = None
            
            if recovered_id in self.tracked_persons:
                self.tracked_persons[recovered_id]['is_tracking'] = True
                self.tracked_persons[recovered_id]['tracking_start_time'] = timestamp
                self.tracked_persons[recovered_id]['last_seen_time'] = timestamp
            
            return recovered_id
        
        self.get_logger().warning(f"ç›®æ ‡ {self.current_tracking_id} ReIDæ‰¾å›å¤±è´¥: æ— åŒ¹é…ç›®æ ‡è¾¾åˆ°é˜ˆå€¼")
        return None

    def _verify_target_with_reid(self, target: TrackedTarget, track: Dict, image: np.ndarray, timestamp: float) -> Optional[int]:
        """ä½¿ç”¨ReIDéªŒè¯ç›®æ ‡èº«ä»½ - å¢åŠ é«˜åº¦ç­›é€‰"""
        track_id = track['track_id']
        bbox = track['bbox']
        
        # è®¡ç®—å½“å‰ç›®æ ‡çš„é«˜åº¦
        x1, y1, x2, y2 = bbox
        candidate_height_pixels = y2 - y1
        target_height_pixels = target.height_pixels
        
        # é«˜åº¦å˜åŒ–ç­›é€‰
        height_ratio = candidate_height_pixels / target_height_pixels
        height_change = abs(1.0 - height_ratio)
        
        # å¦‚æœé«˜åº¦å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œç›´æ¥è¿”å›å¤±è´¥
        if height_change > self.height_change_threshold:
            self.get_logger().warning(f"éªŒè¯ç›®æ ‡ ID:{track_id} é«˜åº¦å˜åŒ– {height_change:.3f} è¶…è¿‡é˜ˆå€¼ {self.height_change_threshold}, éªŒè¯å¤±è´¥")
            return None
        
        candidate_feature = self.extract_feature_from_bbox(image, bbox)
        
        if candidate_feature is not None and np.any(candidate_feature):
            similarity = np.dot(target.feature, candidate_feature) / (
                np.linalg.norm(target.feature) * np.linalg.norm(candidate_feature) + 1e-8
            )
            
            if similarity >= self.reid_similarity_threshold:
                self.get_logger().info(f"ReIDéªŒè¯æˆåŠŸ: ID {track_id}, ç›¸ä¼¼åº¦: {similarity:.3f}, é«˜åº¦å˜åŒ–: {height_change:.3f}")
                
                target.mark_recovered(timestamp)
                # é‡è¦ä¿®æ”¹ï¼šéªŒè¯æˆåŠŸåç«‹å³ç»“æŸæš‚åœçŠ¶æ€
                target.update_paused = False
                target.recovery_time = None
                
                self.save_tracked_target(target.track_id, bbox, image, timestamp)
                self.target_lost_time = None
                
                if track_id in self.tracked_persons:
                    self.tracked_persons[track_id]['is_tracking'] = True
                    self.tracked_persons[track_id]['last_seen_time'] = timestamp
                    if track_id == target.track_id:
                        pass
                    else:
                        self.tracked_persons[track_id]['tracking_start_time'] = timestamp
                
                return track_id
            else:
                self.get_logger().warning(f"ReIDéªŒè¯å¤±è´¥: ID {track_id}, ç›¸ä¼¼åº¦: {similarity:.3f}")

        return None

    def image_callback(self, msg):
        """å›¾åƒå›è°ƒ - ä¼˜åŒ–æ€§èƒ½"""
        current_time = time.time()
        if current_time - self.last_process_time < self.min_process_interval:
            return
        
        self.last_process_time = current_time

        try:
            # å›¾åƒè½¬æ¢
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            
            # YOLOæ¨ç†
            input_tensor = self.model.bgr2nv12(cv_image)
            outputs = self.model.c2numpy(self.model.forward(input_tensor))
            
            # åå¤„ç†
            results = self.model.postProcess(outputs)

            # åˆ†ç¦»æ£€æµ‹ç»“æœ
            person_detections = []
            ok_gestures = []
            stop_gestures = []
            
            for class_id, score, x1, y1, x2, y2 in results:
                if class_id == 0:  # person
                    person_detections.append([x1, y1, x2-x1, y2-y1, score, 0])
                elif class_id == 1:  # okæ‰‹åŠ¿
                    ok_gestures.append((x1, y1, x2, y2, score))
                    self.get_logger().info(f"æ£€æµ‹åˆ°OKæ‰‹åŠ¿: ({x1}, {y1}, {x2}, {y2}), ç½®ä¿¡åº¦: {score:.2f}")
                elif class_id == 2:  # stopæ‰‹åŠ¿
                    stop_gestures.append((x1, y1, x2, y2, score))
                    self.get_logger().info(f"æ£€æµ‹åˆ°STOPæ‰‹åŠ¿: ({x1}, {y1}, {x2}, {y2}), ç½®ä¿¡åº¦: {score:.2f}")

            # è·Ÿè¸ªpersonæ£€æµ‹ç»“æœ
            tracking_results = self.tracker.update(person_detections, cv_image)

            # å¤„ç†è·Ÿè¸ªç»“æœ
            tracks = []
            person_boxes = {}  # å­˜å‚¨personçš„è¾¹ç•Œæ¡†ï¼Œç”¨äºæ‰‹åŠ¿åŒ¹é…
            
            for result in tracking_results:
                x, y, w, h, track_id, score, cls, _, _ = result  # å»æ‰å…³é”®ç‚¹ç›¸å…³å‚æ•°
                x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)
                
                track_data = {
                    'track_id': int(track_id),
                    'bbox': [x1, y1, x2, y2],
                    'conf': float(score),
                }
                tracks.append(track_data)
                person_boxes[track_id] = (x1, y1, x2, y2)

            # æ›´æ–°è·Ÿè¸ªçŠ¶æ€
            self._update_tracking_state(tracks, cv_image, current_time, ok_gestures, stop_gestures, person_boxes)
            
            # æ¸…ç†é•¿æ—¶é—´æœªå‡ºç°çš„è·Ÿè¸ªç›®æ ‡
            self._cleanup_old_tracks(current_time, set(track['track_id'] for track in tracks))

            # å¯è§†åŒ–å¹¶å‘å¸ƒç»“æœ
            self._publish_results(cv_image, tracks, msg.header, ok_gestures, stop_gestures)
                   

        except Exception as e:
            self.get_logger().error(f"Image processing error: {str(e)}")

    def _update_tracking_state(self, tracks: List[Dict], cv_image: np.ndarray, current_time: float, 
                             ok_gestures: List, stop_gestures: List, person_boxes: Dict):
        """æ›´æ–°è·Ÿè¸ªçŠ¶æ€ - æå–ä¸ºç‹¬ç«‹æ–¹æ³•"""
        current_track_ids = set()
        
        for track in tracks:
            track_id = track['track_id']
            current_track_ids.add(track_id)
            
            if track_id not in self.tracked_persons:
                self._initialize_new_track(track_id, current_time)
            else:
                self.tracked_persons[track_id]['last_seen_time'] = current_time

            # å¦‚æœå½“å‰æ­£åœ¨è·Ÿè¸ªè¿™ä¸ªç›®æ ‡ï¼Œä¿å­˜ç‰¹å¾
            if self.current_tracking_id == track_id:
                self.save_tracked_target(track_id, track['bbox'], cv_image, current_time)

        # å¤„ç†æ‰‹åŠ¿æ§åˆ¶é€»è¾‘
        self._process_gesture_control(ok_gestures, stop_gestures, person_boxes, cv_image, current_time)

        # å¤„ç†ä¸¢å¤±ç›®æ ‡
        self._handle_lost_targets(current_track_ids, tracks, cv_image, current_time)

    def _initialize_new_track(self, track_id: int, current_time: float):
        """åˆå§‹åŒ–æ–°è·Ÿè¸ªç›®æ ‡"""
        self.tracked_persons[track_id] = {
            'is_tracking': False,
            'tracking_start_time': 0.0,
            'last_ok_time': 0.0,
            'first_seen_time': current_time,
            'last_seen_time': current_time
        }
        # ç¡®ä¿æ‰‹åŠ¿å†å²è®°å½•è¢«æ­£ç¡®åˆå§‹åŒ–
        if track_id not in self.ok_gesture_history:
            self.ok_gesture_history[track_id] = deque(maxlen=self.ok_confirm_frames)
        if track_id not in self.stop_gesture_history:
            self.stop_gesture_history[track_id] = deque(maxlen=self.ok_confirm_frames)

    def _process_gesture_control(self, ok_gestures: List, stop_gestures: List, 
                               person_boxes: Dict, cv_image: np.ndarray, current_time: float):
        """å¤„ç†æ‰‹åŠ¿æ§åˆ¶é€»è¾‘ - ä¿®å¤ç‰ˆæœ¬"""
        # å¤„ç†okæ‰‹åŠ¿æ£€æµ‹
        if ok_gestures:
            self.get_logger().info(f"å¼€å§‹å¤„ç† {len(ok_gestures)} ä¸ªOKæ‰‹åŠ¿")
            
        for ok_gesture in ok_gestures:
            x1, y1, x2, y2, score = ok_gesture
            ok_box = (x1, y1, x2, y2)
            
            # æ‰¾åˆ°ä¸okæ‰‹åŠ¿é‡å åº¦æœ€é«˜çš„äººä½“
            person_id, person_box, iou = self.find_person_for_gesture(ok_box, person_boxes)
            
            if person_id is not None:
                self.get_logger().info(f"OKæ‰‹åŠ¿ä¸ID {person_id} çš„IoU: {iou:.3f} (é˜ˆå€¼: {self.roi_threshold})")
                
                if iou >= self.roi_threshold:
                    # ç¡®ä¿æ‰‹åŠ¿å†å²è®°å½•å­˜åœ¨
                    if person_id not in self.ok_gesture_history:
                        self.ok_gesture_history[person_id] = deque(maxlen=self.ok_confirm_frames)
                    
                    # æ·»åŠ æ‰‹åŠ¿æ£€æµ‹è®°å½•
                    self.ok_gesture_history[person_id].append(True)
                    current_count = len(self.ok_gesture_history[person_id])
                    ok_confirmed = current_count >= self.ok_confirm_frames
                    
                    self.get_logger().info(f"ID {person_id} OKæ‰‹åŠ¿å†å²: {current_count}/{self.ok_confirm_frames}")
                    
                    if ok_confirmed:
                        self._handle_ok_gesture(person_id, person_box, cv_image, current_time)
                else:
                    self.get_logger().warning(f"OKæ‰‹åŠ¿ä¸ID {person_id} çš„IoU {iou:.3f} ä½äºé˜ˆå€¼ {self.roi_threshold}")
            else:
                self.get_logger().warning("æœªæ‰¾åˆ°ä¸OKæ‰‹åŠ¿åŒ¹é…çš„äººå‘˜")

        # å¤„ç†stopæ‰‹åŠ¿æ£€æµ‹
        for stop_gesture in stop_gestures:
            x1, y1, x2, y2, score = stop_gesture
            stop_box = (x1, y1, x2, y2)
            
            # æ‰¾åˆ°ä¸stopæ‰‹åŠ¿é‡å åº¦æœ€é«˜çš„äººä½“
            person_id, person_box, iou = self.find_person_for_gesture(stop_box, person_boxes)
            
            if person_id is not None and iou >= self.roi_threshold:
                # ç¡®ä¿æ‰‹åŠ¿å†å²è®°å½•å­˜åœ¨
                if person_id not in self.stop_gesture_history:
                    self.stop_gesture_history[person_id] = deque(maxlen=self.ok_confirm_frames)
                
                # æ·»åŠ æ‰‹åŠ¿æ£€æµ‹è®°å½•
                self.stop_gesture_history[person_id].append(True)
                current_count = len(self.stop_gesture_history[person_id])
                stop_confirmed = current_count >= self.ok_confirm_frames
                
                self.get_logger().info(f"ID {person_id} STOPæ‰‹åŠ¿å†å²: {current_count}/{self.ok_confirm_frames}")
                
                if stop_confirmed:
                    self._handle_stop_gesture(person_id, current_time)

    def _handle_ok_gesture(self, person_id: int, person_box: Tuple, cv_image: np.ndarray, current_time: float):
        """å¤„ç†okæ‰‹åŠ¿ç¡®è®¤"""
        person = self.tracked_persons[person_id]
        in_cooldown_period = (current_time - person['last_ok_time'] < self.tracking_protection_time)
        
        if not in_cooldown_period:
            self.current_tracking_id = person_id
            person['is_tracking'] = True
            person['tracking_start_time'] = current_time
            person['last_ok_time'] = current_time
            
            # æ¸…ç©ºæ‰‹åŠ¿å†å²
            if person_id in self.ok_gesture_history:
                self.ok_gesture_history[person_id].clear()
            
            self.save_tracked_target(person_id, list(person_box), cv_image, current_time)
            self.target_lost_time = None
            self.get_logger().info(f"ğŸ¯ å¼€å§‹è·Ÿè¸ª ID: {person_id} (OKæ‰‹åŠ¿ç¡®è®¤)")

    def _handle_stop_gesture(self, person_id: int, current_time: float):
        """å¤„ç†stopæ‰‹åŠ¿ç¡®è®¤"""
        if self.current_tracking_id == person_id:
            person = self.tracked_persons[person_id]
            in_protection_period = (current_time - person['tracking_start_time'] < self.tracking_protection_time)
            
            if not in_protection_period:
                person['is_tracking'] = False
                person['last_ok_time'] = current_time
                
                # æ¸…ç©ºæ‰‹åŠ¿å†å²
                if person_id in self.stop_gesture_history:
                    self.stop_gesture_history[person_id].clear()
                
                self.current_tracking_id = None
                self.target_lost_time = None
                self.get_logger().info(f"ğŸ›‘ åœæ­¢è·Ÿè¸ª ID: {person_id}")

    def _handle_lost_targets(self, current_track_ids: set, tracks: List[Dict], 
                            cv_image: np.ndarray, current_time: float):
        """å¤„ç†ä¸¢å¤±ç›®æ ‡ - æ·»åŠ è‡ªåŠ¨æ‰¾å›åŠŸèƒ½"""
        if self.current_tracking_id is not None and self.current_tracking_id not in current_track_ids:
            if self.current_tracking_id in self.tracked_targets:
                self.tracked_targets[self.current_tracking_id].mark_lost()
                
                if self.current_tracking_id in self.tracked_persons:
                    self.tracked_persons[self.current_tracking_id]['is_tracking'] = False
                
                # ç¡®ä¿ä¸¢å¤±æ—¶é—´è¢«æ­£ç¡®è®¾ç½®
                if self.target_lost_time is None:
                    self.target_lost_time = current_time
                    self.get_logger().warning(f"ç›®æ ‡ {self.current_tracking_id} ä¸¢å¤±ï¼Œç«‹å³å¯åŠ¨ReIDåŒ¹é…æ‰¾å›")
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                time_since_lost = current_time - self.target_lost_time
                
                if time_since_lost > self.lost_timeout_threshold:
                    self.get_logger().warning(
                        f"ç›®æ ‡ {self.current_tracking_id} ä¸¢å¤±è¶…è¿‡ {self.lost_timeout_threshold} ç§’ï¼Œåœæ­¢è·Ÿè¸ªå¹¶æ¸…é™¤ç›®æ ‡ä¿¡æ¯ï¼Œéœ€è¦é‡æ–°OKæ‰‹åŠ¿é€‰æ‹©è·Ÿè¸ªç›®æ ‡"
                    )
                    self._clear_tracking_target()
                    return
                
                recovered_id = self.try_recover_lost_target(tracks, cv_image, current_time)
                if recovered_id is not None:
                    # é‡è¦ï¼šåœ¨è®¾ç½®æ–°IDå‰ï¼Œæ¸…ç†åŸç›®æ ‡çš„è·Ÿè¸ªçŠ¶æ€
                    old_tracking_id = self.current_tracking_id
                    self.current_tracking_id = recovered_id
                    
                    # ç¡®ä¿æ–°ç›®æ ‡è¢«æ­£ç¡®æ ‡è®°ä¸ºè·Ÿè¸ªçŠ¶æ€
                    if recovered_id in self.tracked_persons:
                        self.tracked_persons[recovered_id]['is_tracking'] = True
                        self.tracked_persons[recovered_id]['tracking_start_time'] = current_time
                        self.tracked_persons[recovered_id]['last_seen_time'] = current_time
                    
                    # é‡ç½®ä¸¢å¤±æ—¶é—´
                    self.target_lost_time = None
                    
                    # ä¿å­˜æ–°ç›®æ ‡çš„ç‰¹å¾ä¿¡æ¯
                    if recovered_id in [t['track_id'] for t in tracks]:
                        track = next(t for t in tracks if t['track_id'] == recovered_id)
                        self.save_tracked_target(recovered_id, track['bbox'], cv_image, current_time)
                    
                    self.get_logger().info(f"ReIDæ‰¾å›æˆåŠŸï¼Œä»ID {old_tracking_id} åˆ‡æ¢åˆ°æ–°ID: {recovered_id}")
                else:
                    self.get_logger().warning(f"ç›®æ ‡ {self.current_tracking_id} ReIDæ‰¾å›å¤±è´¥ï¼Œä¿æŒä¸¢å¤±çŠ¶æ€")
                    
                self.get_logger().info(f"======================================================")
        
        elif self.current_tracking_id is not None and self.current_tracking_id in current_track_ids:
            if self.target_lost_time is not None:
                track = next(t for t in tracks if t['track_id'] == self.current_tracking_id)
                verified_id = self._verify_target_with_reid(
                    self.tracked_targets[self.current_tracking_id], track, cv_image, current_time
                )
                
                if verified_id is not None:
                    # ReIDéªŒè¯æˆåŠŸï¼Œé‡ç½®ä¸¢å¤±æ—¶é—´
                    self.target_lost_time = None
                    self.get_logger().info(f"ç›®æ ‡ {self.current_tracking_id}é‡æ–°å‡ºç°ï¼ŒReIDéªŒè¯æˆåŠŸï¼Œç»§ç»­è·Ÿè¸ª")
                    
                    if verified_id in self.tracked_persons:
                        self.tracked_persons[verified_id]['is_tracking'] = True
                        self.tracked_persons[verified_id]['last_seen_time'] = current_time
                else:
                    # ReIDéªŒè¯å¤±è´¥ï¼Œä½†ç›®æ ‡é‡æ–°å‡ºç° - å…³é”®ä¿®æ”¹ï¼šç»§ç»­ç´¯ç§¯ä¸¢å¤±æ—¶é—´
                    time_since_lost = current_time - self.target_lost_time

                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if time_since_lost > self.lost_timeout_threshold:
                        self.get_logger().warning(
                            f"ç›®æ ‡ {self.current_tracking_id} ReIDéªŒè¯å¤±è´¥è¶…è¿‡ {self.lost_timeout_threshold} ç§’ï¼Œåœæ­¢è·Ÿè¸ªå¹¶æ¸…é™¤ç›®æ ‡ä¿¡æ¯ï¼Œéœ€è¦é‡æ–°OKæ‰‹åŠ¿é€‰æ‹©è·Ÿè¸ªç›®æ ‡"
                        )
                        self._clear_tracking_target()
                        return
                    
                self.get_logger().info(f"======================================================")

    def _clear_tracking_target(self):
        """æ¸…é™¤å½“å‰è·Ÿè¸ªç›®æ ‡çš„æ‰€æœ‰ä¿¡æ¯"""
        if self.current_tracking_id is not None:
            target_id = self.current_tracking_id          
            # æ¸…é™¤æ‰€æœ‰ç›¸å…³å­˜å‚¨
            if target_id in self.tracked_persons:
                del self.tracked_persons[target_id]
            if target_id in self.ok_gesture_history:
                del self.ok_gesture_history[target_id]
            if target_id in self.stop_gesture_history:
                del self.stop_gesture_history[target_id]
            if target_id in self.tracked_targets:
                del self.tracked_targets[target_id]
      
        # é‡ç½®è·Ÿè¸ªçŠ¶æ€
        self.current_tracking_id = None
        self.target_lost_time = None
        
    def _cleanup_old_tracks(self, current_time: float, current_track_ids: set):
        """æ¸…ç†é•¿æ—¶é—´æœªå‡ºç°çš„è·Ÿè¸ªç›®æ ‡"""
        max_track_age = 5.0
        
        for track_id in list(self.tracked_persons.keys()):
            if track_id == self.current_tracking_id:
                continue
                
            if track_id not in current_track_ids:
                last_seen = self.tracked_persons[track_id]['last_seen_time']
                if current_time - last_seen > max_track_age:
                    self._remove_track(track_id)

    def _remove_track(self, track_id: int):
        """ç§»é™¤è·Ÿè¸ªç›®æ ‡"""
        if track_id in self.tracked_persons:
            del self.tracked_persons[track_id]
        if track_id in self.ok_gesture_history:
            del self.ok_gesture_history[track_id]
        if track_id in self.stop_gesture_history:
            del self.stop_gesture_history[track_id]
        if track_id in self.tracked_targets:
            target = self.tracked_targets[track_id]
            if target.is_switched and target.original_track_id not in self.tracked_targets:
                original_id = target.original_track_id
                self.tracked_targets[original_id] = copy.deepcopy(target)
                self.tracked_targets[original_id].track_id = original_id
                self.tracked_targets[original_id].is_switched = False
            
            del self.tracked_targets[track_id]
        
    def _publish_results(self, image: np.ndarray, tracks: List[Dict], header, ok_gestures, stop_gestures):
        """å‘å¸ƒç»“æœ - åˆå¹¶å¯è§†åŒ–"""
        self.visualize_results(image, tracks, ok_gestures, stop_gestures)
        self.publish_tracked_keypoints(tracks, header)

        # åªåœ¨æœ‰è®¢é˜…è€…æ—¶æ‰è¿›è¡Œå¯è§†åŒ–å‘å¸ƒ
        if self.detect_pose_pub.get_subscription_count() > 0:
            detect_pose_msg = self.bridge.cv2_to_imgmsg(image, encoding='bgr8')
            detect_pose_msg.header = header
            self.detect_pose_pub.publish(detect_pose_msg)

    def visualize_results(self, image: np.ndarray, tracks: List[Dict], ok_gestures, stop_gestures):
        """ç®€åŒ–ç‰ˆå¯è§†åŒ–è·Ÿè¸ªç»“æœ - ä¼˜åŒ–ç»˜åˆ¶æ€§èƒ½"""
        display_image = image.copy()
        
        # ç»˜åˆ¶æ‰‹åŠ¿æ£€æµ‹ç»“æœ
        for ok_gesture in ok_gestures:
            x1, y1, x2, y2, score = ok_gesture
            cv2.rectangle(display_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f"OK: {score:.2f}"
            cv2.putText(display_image, label, (x1, y1 - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        for stop_gesture in stop_gestures:
            x1, y1, x2, y2, score = stop_gesture
            cv2.rectangle(display_image, (x1, y1), (x2, y2), (0, 0, 255), 2)
            label = f"STOP: {score:.2f}"
            cv2.putText(display_image, label, (x1, y1 - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
        
        for track in tracks:
            track_id = track['track_id']
            x1, y1, x2, y2 = track['bbox']
            confidence = track['conf']

            # ç¡®å®šè·Ÿè¸ªçŠ¶æ€å’Œé¢œè‰²
            is_tracking = (track_id == self.current_tracking_id and 
                        track_id in self.tracked_persons and 
                        self.tracked_persons[track_id]['is_tracking'])
            
            color = (255, 0, 0) if is_tracking else (0, 255, 0)
            thickness = 3 if is_tracking else 2

            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(display_image, (x1, y1), (x2, y2), color, thickness)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"ID:{track_id} {confidence:.2f}"
            if is_tracking:
                label = f"TRACKING {label}"
            
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(display_image, (x1, y1 - label_size[1] - 10), 
                         (x1 + label_size[0], y1), color, -1)
            cv2.putText(display_image, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # æ›´æ–°å›¾åƒ
        image[:] = display_image

    def publish_tracked_keypoints(self, tracks: List[Dict], header):
        """å‘å¸ƒè¾¹ç•Œæ¡†å’Œè‚©éƒ¨å…³é”®ç‚¹åæ ‡å’Œç½®ä¿¡åº¦ - ç®€åŒ–ä¼˜åŒ–ç‰ˆ"""
        current_tracking_id = self.current_tracking_id
        
        # æŸ¥æ‰¾å½“å‰è·Ÿè¸ªçš„ç›®æ ‡
        tracking_target = None
        for track in tracks:
            track_id = track['track_id']
            if (current_tracking_id is not None and track_id == current_tracking_id) or \
            (track_id in self.tracked_persons and self.tracked_persons[track_id]['is_tracking']):
                tracking_target = track
                break
        
        polygon_msg = PolygonStamped()
        polygon_msg.header = header
        polygon_msg.header.frame_id = "camera_link"
        
        if tracking_target:
            # å‘å¸ƒæ­£å¸¸è·Ÿè¸ªçŠ¶æ€
            track_id = tracking_target['track_id']
            x1, y1, x2, y2 = tracking_target['bbox']
            
            # æ„å»ºæ¶ˆæ¯ç‚¹ï¼šçŠ¶æ€ä¿¡æ¯ + è¾¹ç•Œæ¡†
            points = [
                Point32(x=float(track_id), y=1.0, z=2.0),  # çŠ¶æ€ç‚¹
                Point32(x=float(x1), y=float(y1), z=0.0),   # è¾¹ç•Œæ¡†å·¦ä¸Š
                Point32(x=float(x2), y=float(y2), z=0.0),   # è¾¹ç•Œæ¡†å³ä¸‹
            ]
            
            polygon_msg.polygon.points = points
            # self.get_logger().info(f"ğŸ“¤ å‘å¸ƒè·Ÿè¸ªä¿¡æ¯: ID {track_id}")
            
        elif current_tracking_id is not None:
            # å‘å¸ƒç›®æ ‡ä¸¢å¤±çŠ¶æ€
            points = [
                Point32(x=float(current_tracking_id), y=0.0, z=0.0),  # çŠ¶æ€ç‚¹ï¼šy=0è¡¨ç¤ºä¸¢å¤±
                Point32(x=0.0, y=0.0, z=0.0),
                Point32(x=0.0, y=0.0, z=0.0),
            ]
            polygon_msg.polygon.points = points
            self.get_logger().info(f"ğŸ“¤ å‘å¸ƒç›®æ ‡ä¸¢å¤±çŠ¶æ€: ID {current_tracking_id}")
        
        else:
            # æ— è·Ÿè¸ªç›®æ ‡çŠ¶æ€
            points = [Point32(x=0.0, y=0.0, z=0.0) for _ in range(3)]  # 3ä¸ªé›¶å€¼ç‚¹
            polygon_msg.polygon.points = points
        
        # å‘å¸ƒæ¶ˆæ¯
        self.keypoint_tracks_pub.publish(polygon_msg)

def main(args=None):
    rclpy.init(args=args)
    
    try:
        node = Yolov8HandTrackNode()
        executor = MultiThreadedExecutor()
        executor.add_node(node)
        
        try:
            executor.spin()
        finally:
            executor.shutdown()
            node.destroy_node()
            
    except Exception as e:
        print(f"Node initialization failed: {e}")
    finally:
        rclpy.shutdown()

if __name__ == '__main__':
    main()